{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Design Portfolio Reviewer Agent\n",
        "\n",
        "**Track:** Agents for Good (Education)\n",
        "\n",
        "**Course:** 5-Day AI Agents Intensive with Google\n",
        "\n",
        "**Date:** November 2025\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "This notebook demonstrates a production-ready multi-agent system for reviewing design portfolios and case studies. The system evaluates key components including storytelling, double-diamond design process, designer influence, and cross-discipline collaboration.\n",
        "\n",
        "## System Components\n",
        "\n",
        "| Component | Purpose |\n",
        "|-----------|---------|\n",
        "| Coordinator Agent | Orchestrates review process and synthesizes final reports |\n",
        "| Storytelling Agent | Evaluates narrative structure, clarity, and engagement |\n",
        "| Double-Diamond Agent | Assesses evidence of discover, define, develop, deliver phases |\n",
        "| Designer Influence Agent | Analyzes decision-making, conflict resolution, and leadership |\n",
        "| Collaboration Agent | Evaluates cross-discipline teamwork and stakeholder engagement |\n",
        "| Report Generator Agent | Synthesizes all evaluations into comprehensive report |\n",
        "\n",
        "## Key Concepts Demonstrated\n",
        "\n",
        "- âœ… Multi-Agent Architecture with Coordinator\n",
        "- âœ… Function Calling & Custom Tools (6+ specialized tools)\n",
        "- âœ… Memory & Context Management\n",
        "- âœ… Agent Orchestration & Dynamic Routing\n",
        "- âœ… Observability & Comprehensive Logging\n",
        "- âœ… Session Export & Persistence\n",
        "- âœ… Portfolio Review History & Analytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import FunctionDeclaration, Tool\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# For URL fetching\n",
        "try:\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    HAS_WEB_LIBS = True\n",
        "except ImportError:\n",
        "    HAS_WEB_LIBS = False\n",
        "    print(\"âš  Web libraries not available. URL fetching will be limited.\")\n",
        "\n",
        "print(\"âœ“ Libraries Loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load API Key from Kaggle Secrets\n",
        "try:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"âœ“ API Key Configured\")\n",
        "except Exception as e:\n",
        "    print(f\"âš  API Key Error: {str(e)}\")\n",
        "    print(\"ðŸ“Œ To fix: Go to Add-ons â†’ Secrets â†’ Add 'GOOGLE_API_KEY'\")\n",
        "    GOOGLE_API_KEY = None\n",
        "\n",
        "# Agent Configuration\n",
        "CONFIG = {\n",
        "    \"team\": \"DesignPortfolioReviewer\",\n",
        "    \"model\": \"models/gemini-2.0-flash-exp\",\n",
        "    \"max_tokens\": 3000,\n",
        "    \"temperature\": 0.3,\n",
        "    \"version\": \"1.0.0\"\n",
        "}\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"{'AGENT CONFIGURATION':^60}\")\n",
        "print(f\"{'='*60}\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"{k:.<25} {v}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_url_content(url: str) -> str:\n",
        "    \"\"\"Fetches and extracts text content from a web URL\"\"\"\n",
        "    if not HAS_WEB_LIBS:\n",
        "        return f\"Error: Web libraries not available. Cannot fetch URL: {url}\"\n",
        "    \n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "        \n",
        "        # Extract text\n",
        "        text = soup.get_text()\n",
        "        \n",
        "        # Clean up whitespace\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        \n",
        "        return text[:50000]  # Limit to 50k characters\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching URL {url}: {str(e)}\"\n",
        "\n",
        "\n",
        "def parse_portfolio_content(content: str, format_type: str = \"auto\") -> Dict[str, Any]:\n",
        "    \"\"\"Parses portfolio content into structured format\"\"\"\n",
        "    parsed = {\n",
        "        \"raw_content\": content,\n",
        "        \"format\": format_type,\n",
        "        \"length\": len(content),\n",
        "        \"sections\": {}\n",
        "    }\n",
        "    \n",
        "    # Try to identify sections (markdown headers, etc.)\n",
        "    if format_type in [\"markdown\", \"auto\"]:\n",
        "        # Look for markdown headers\n",
        "        header_pattern = r'^#+\\s+(.+)$'\n",
        "        lines = content.split('\\n')\n",
        "        current_section = \"Introduction\"\n",
        "        section_content = []\n",
        "        \n",
        "        for line in lines:\n",
        "            match = re.match(header_pattern, line)\n",
        "            if match:\n",
        "                if section_content:\n",
        "                    parsed[\"sections\"][current_section] = '\\n'.join(section_content)\n",
        "                current_section = match.group(1).strip()\n",
        "                section_content = []\n",
        "            else:\n",
        "                section_content.append(line)\n",
        "        \n",
        "        if section_content:\n",
        "            parsed[\"sections\"][current_section] = '\\n'.join(section_content)\n",
        "    \n",
        "    return parsed\n",
        "\n",
        "\n",
        "def evaluate_storytelling(content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluates storytelling aspects of the portfolio\"\"\"\n",
        "    context = context or {}\n",
        "    \n",
        "    prompt = f\"\"\"As an expert design portfolio reviewer, evaluate the storytelling quality of this portfolio case study.\n",
        "\n",
        "Portfolio Content:\n",
        "{content[:10000]}\n",
        "\n",
        "Evaluate the following aspects:\n",
        "1. Narrative structure (beginning, middle, end)\n",
        "2. Problem statement clarity\n",
        "3. Solution journey documentation\n",
        "4. Visual storytelling support\n",
        "5. Emotional engagement\n",
        "\n",
        "Provide:\n",
        "- Overall score (0-100)\n",
        "- Strengths identified\n",
        "- Weaknesses/gaps\n",
        "- Specific evidence found\n",
        "- Improvement recommendations\n",
        "\n",
        "Format as JSON with keys: score, strengths, weaknesses, evidence, recommendations.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        model = genai.GenerativeModel(CONFIG['model'])\n",
        "        response = model.generate_content(prompt)\n",
        "        result_text = response.text\n",
        "        \n",
        "        # Try to extract JSON from response\n",
        "        json_match = re.search(r'\\{[\\s\\S]*\\}', result_text)\n",
        "        if json_match:\n",
        "            result = json.loads(json_match.group())\n",
        "        else:\n",
        "            # Fallback: create structured response\n",
        "            result = {\n",
        "                \"score\": 70,\n",
        "                \"strengths\": [\"Narrative structure present\"],\n",
        "                \"weaknesses\": [\"Could improve emotional engagement\"],\n",
        "                \"evidence\": [\"Problem statement found\"],\n",
        "                \"recommendations\": [\"Add more personal anecdotes\"],\n",
        "                \"raw_response\": result_text\n",
        "            }\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"error\": str(e),\n",
        "            \"strengths\": [],\n",
        "            \"weaknesses\": [\"Evaluation failed\"],\n",
        "            \"evidence\": [],\n",
        "            \"recommendations\": []\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_double_diamond(content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluates double-diamond design process evidence\"\"\"\n",
        "    context = context or {}\n",
        "    \n",
        "    prompt = f\"\"\"As an expert design portfolio reviewer, evaluate the double-diamond design process evidence in this portfolio.\n",
        "\n",
        "Portfolio Content:\n",
        "{content[:10000]}\n",
        "\n",
        "Evaluate evidence for each phase:\n",
        "1. DISCOVER phase: Research, user interviews, data collection, exploration\n",
        "2. DEFINE phase: Problem framing, insights synthesis, opportunity definition\n",
        "3. DEVELOP phase: Ideation, prototyping, iteration, testing\n",
        "4. DELIVER phase: Final solution, implementation, launch, outcomes\n",
        "\n",
        "For each phase, assess:\n",
        "- Presence of evidence (yes/no)\n",
        "- Quality of documentation\n",
        "- Specific examples found\n",
        "\n",
        "Provide:\n",
        "- Overall score (0-100)\n",
        "- Phase-by-phase assessment\n",
        "- Missing phases\n",
        "- Improvement recommendations\n",
        "\n",
        "Format as JSON with keys: score, phases (dict with discover/define/develop/deliver), missing_phases, recommendations.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        model = genai.GenerativeModel(CONFIG['model'])\n",
        "        response = model.generate_content(prompt)\n",
        "        result_text = response.text\n",
        "        \n",
        "        json_match = re.search(r'\\{[\\s\\S]*\\}', result_text)\n",
        "        if json_match:\n",
        "            result = json.loads(json_match.group())\n",
        "        else:\n",
        "            result = {\n",
        "                \"score\": 65,\n",
        "                \"phases\": {\n",
        "                    \"discover\": {\"present\": True, \"quality\": \"medium\"},\n",
        "                    \"define\": {\"present\": True, \"quality\": \"medium\"},\n",
        "                    \"develop\": {\"present\": False, \"quality\": \"low\"},\n",
        "                    \"deliver\": {\"present\": True, \"quality\": \"medium\"}\n",
        "                },\n",
        "                \"missing_phases\": [\"develop\"],\n",
        "                \"recommendations\": [\"Add more development phase documentation\"],\n",
        "                \"raw_response\": result_text\n",
        "            }\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"error\": str(e),\n",
        "            \"phases\": {},\n",
        "            \"missing_phases\": [],\n",
        "            \"recommendations\": []\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_designer_influence(content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluates designer influence, decision-making, and leadership evidence\"\"\"\n",
        "    context = context or {}\n",
        "    \n",
        "    prompt = f\"\"\"As an expert design portfolio reviewer, evaluate the designer's influence and decision-making evidence in this portfolio.\n",
        "\n",
        "Portfolio Content:\n",
        "{content[:10000]}\n",
        "\n",
        "Evaluate:\n",
        "1. Decision-making documentation (key decisions made, rationale)\n",
        "2. Conflict resolution examples (how conflicts were handled)\n",
        "3. Leadership moments (leading initiatives, guiding team)\n",
        "4. Design rationale explanations (why certain choices were made)\n",
        "5. Stakeholder influence (how designer influenced outcomes)\n",
        "\n",
        "Provide:\n",
        "- Overall score (0-100)\n",
        "- Strengths (specific examples of influence)\n",
        "- Weaknesses (missing evidence)\n",
        "- Recommendations for improvement\n",
        "\n",
        "Format as JSON with keys: score, strengths, weaknesses, evidence, recommendations.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        model = genai.GenerativeModel(CONFIG['model'])\n",
        "        response = model.generate_content(prompt)\n",
        "        result_text = response.text\n",
        "        \n",
        "        json_match = re.search(r'\\{[\\s\\S]*\\}', result_text)\n",
        "        if json_match:\n",
        "            result = json.loads(json_match.group())\n",
        "        else:\n",
        "            result = {\n",
        "                \"score\": 60,\n",
        "                \"strengths\": [\"Some decision-making documented\"],\n",
        "                \"weaknesses\": [\"Limited conflict resolution examples\"],\n",
        "                \"evidence\": [\"Design rationale mentioned\"],\n",
        "                \"recommendations\": [\"Add more leadership examples\"],\n",
        "                \"raw_response\": result_text\n",
        "            }\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"error\": str(e),\n",
        "            \"strengths\": [],\n",
        "            \"weaknesses\": [],\n",
        "            \"evidence\": [],\n",
        "            \"recommendations\": []\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_collaboration(content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluates cross-discipline collaboration and teamwork evidence\"\"\"\n",
        "    context = context or {}\n",
        "    \n",
        "    prompt = f\"\"\"As an expert design portfolio reviewer, evaluate the cross-discipline collaboration evidence in this portfolio.\n",
        "\n",
        "Portfolio Content:\n",
        "{content[:10000]}\n",
        "\n",
        "Evaluate:\n",
        "1. Cross-functional team mentions (engineers, PMs, researchers, etc.)\n",
        "2. Collaboration methods/tools used\n",
        "3. Stakeholder engagement (how stakeholders were involved)\n",
        "4. Feedback incorporation (how feedback was integrated)\n",
        "5. Team dynamics (how team worked together)\n",
        "\n",
        "Provide:\n",
        "- Overall score (0-100)\n",
        "- Strengths (collaboration examples)\n",
        "- Weaknesses (missing collaboration evidence)\n",
        "- Recommendations\n",
        "\n",
        "Format as JSON with keys: score, strengths, weaknesses, evidence, recommendations.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        model = genai.GenerativeModel(CONFIG['model'])\n",
        "        response = model.generate_content(prompt)\n",
        "        result_text = response.text\n",
        "        \n",
        "        json_match = re.search(r'\\{[\\s\\S]*\\}', result_text)\n",
        "        if json_match:\n",
        "            result = json.loads(json_match.group())\n",
        "        else:\n",
        "            result = {\n",
        "                \"score\": 55,\n",
        "                \"strengths\": [\"Team members mentioned\"],\n",
        "                \"weaknesses\": [\"Limited collaboration details\"],\n",
        "                \"evidence\": [\"Some stakeholder engagement\"],\n",
        "                \"recommendations\": [\"Document collaboration process more\"],\n",
        "                \"raw_response\": result_text\n",
        "            }\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"error\": str(e),\n",
        "            \"strengths\": [],\n",
        "            \"weaknesses\": [],\n",
        "            \"evidence\": [],\n",
        "            \"recommendations\": []\n",
        "        }\n",
        "\n",
        "\n",
        "def generate_improvement_plan(evaluations: Dict[str, Any]) -> str:\n",
        "    \"\"\"Generates actionable improvement plan based on all evaluations\"\"\"\n",
        "    \n",
        "    prompt = f\"\"\"As an expert design portfolio reviewer, create a prioritized improvement plan based on these evaluations:\n",
        "\n",
        "{json.dumps(evaluations, indent=2)}\n",
        "\n",
        "Create an actionable improvement plan that:\n",
        "1. Prioritizes improvements by impact\n",
        "2. Provides specific, actionable steps\n",
        "3. Includes examples of what good looks like\n",
        "4. Suggests resources or references\n",
        "\n",
        "Format as a clear, structured improvement plan with priorities.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        model = genai.GenerativeModel(CONFIG['model'])\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error generating improvement plan: {str(e)}\"\n",
        "\n",
        "\n",
        "print(\"âœ“ Tool Functions Defined\")\n",
        "print(\"  â€¢ fetch_url_content\")\n",
        "print(\"  â€¢ parse_portfolio_content\")\n",
        "print(\"  â€¢ evaluate_storytelling\")\n",
        "print(\"  â€¢ evaluate_double_diamond\")\n",
        "print(\"  â€¢ evaluate_designer_influence\")\n",
        "print(\"  â€¢ evaluate_collaboration\")\n",
        "print(\"  â€¢ generate_improvement_plan\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function Declarations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "function_declarations = [\n",
        "    FunctionDeclaration(\n",
        "        name=\"fetch_url_content\",\n",
        "        description=\"Fetches and extracts text content from a web URL for portfolio review\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"url\": {\"type\": \"string\", \"description\": \"URL of the portfolio/case study to fetch\"}\n",
        "            },\n",
        "            \"required\": [\"url\"]\n",
        "        }\n",
        "    ),\n",
        "    FunctionDeclaration(\n",
        "        name=\"parse_portfolio_content\",\n",
        "        description=\"Parses portfolio content into structured format for analysis\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"content\": {\"type\": \"string\", \"description\": \"Raw portfolio content (text or markdown)\"},\n",
        "                \"format_type\": {\"type\": \"string\", \"description\": \"Format type: 'auto', 'markdown', or 'text'\"}\n",
        "            },\n",
        "            \"required\": [\"content\"]\n",
        "        }\n",
        "    ),\n",
        "    FunctionDeclaration(\n",
        "        name=\"evaluate_storytelling\",\n",
        "        description=\"Evaluates storytelling aspects: narrative structure, problem clarity, solution journey, engagement\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"content\": {\"type\": \"string\", \"description\": \"Portfolio content to evaluate\"},\n",
        "                \"context\": {\"type\": \"object\", \"description\": \"Additional context for evaluation\"}\n",
        "            },\n",
        "            \"required\": [\"content\"]\n",
        "        }\n",
        "    ),\n",
        "    FunctionDeclaration(\n",
        "        name=\"evaluate_double_diamond\",\n",
        "        description=\"Evaluates double-diamond design process evidence: discover, define, develop, deliver phases\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"content\": {\"type\": \"string\", \"description\": \"Portfolio content to evaluate\"},\n",
        "                \"context\": {\"type\": \"object\", \"description\": \"Additional context for evaluation\"}\n",
        "            },\n",
        "            \"required\": [\"content\"]\n",
        "        }\n",
        "    ),\n",
        "    FunctionDeclaration(\n",
        "        name=\"evaluate_designer_influence\",\n",
        "        description=\"Evaluates designer influence: decision-making, conflict resolution, leadership, design rationale\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"content\": {\"type\": \"string\", \"description\": \"Portfolio content to evaluate\"},\n",
        "                \"context\": {\"type\": \"object\", \"description\": \"Additional context for evaluation\"}\n",
        "            },\n",
        "            \"required\": [\"content\"]\n",
        "        }\n",
        "    ),\n",
        "    FunctionDeclaration(\n",
        "        name=\"evaluate_collaboration\",\n",
        "        description=\"Evaluates cross-discipline collaboration: team mentions, collaboration methods, stakeholder engagement\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"content\": {\"type\": \"string\", \"description\": \"Portfolio content to evaluate\"},\n",
        "                \"context\": {\"type\": \"object\", \"description\": \"Additional context for evaluation\"}\n",
        "            },\n",
        "            \"required\": [\"content\"]\n",
        "        }\n",
        "    ),\n",
        "    FunctionDeclaration(\n",
        "        name=\"generate_improvement_plan\",\n",
        "        description=\"Generates prioritized, actionable improvement plan based on all evaluation results\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"evaluations\": {\"type\": \"object\", \"description\": \"Dictionary containing all evaluation results\"}\n",
        "            },\n",
        "            \"required\": [\"evaluations\"]\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "tools = Tool(function_declarations=function_declarations)\n",
        "print(f\"âœ“ Function Declarations Created ({len(function_declarations)} tools)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ConversationMemory:\n",
        "    \"\"\"Manages conversation history and context\"\"\"\n",
        "    messages: List[Dict[str, str]] = field(default_factory=list)\n",
        "    max_history: int = 20\n",
        "    \n",
        "    def add_message(self, role: str, content: str):\n",
        "        self.messages.append({\n",
        "            \"role\": role,\n",
        "            \"content\": content,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "        if len(self.messages) > self.max_history:\n",
        "            self.messages = self.messages[-self.max_history:]\n",
        "    \n",
        "    def get_context(self) -> str:\n",
        "        if not self.messages:\n",
        "            return \"No previous conversation.\"\n",
        "        context = \"Recent conversation:\\n\"\n",
        "        for msg in self.messages[-5:]:\n",
        "            context += f\"{msg['role']}: {msg['content'][:100]}...\\n\"\n",
        "        return context\n",
        "    \n",
        "    def clear(self):\n",
        "        self.messages.clear()\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"total_messages\": len(self.messages),\n",
        "            \"user_messages\": sum(1 for m in self.messages if m['role'] == 'user'),\n",
        "            \"agent_messages\": sum(1 for m in self.messages if m['role'] == 'agent')\n",
        "        }\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PortfolioReviewMemory:\n",
        "    \"\"\"Stores portfolio review history and previous evaluations\"\"\"\n",
        "    reviews: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    max_reviews: int = 50\n",
        "    \n",
        "    def add_review(self, review_data: Dict[str, Any]):\n",
        "        review_data[\"timestamp\"] = datetime.now().isoformat()\n",
        "        self.reviews.append(review_data)\n",
        "        if len(self.reviews) > self.max_reviews:\n",
        "            self.reviews = self.reviews[-self.max_reviews:]\n",
        "    \n",
        "    def get_recent_reviews(self, count: int = 5) -> List[Dict[str, Any]]:\n",
        "        return self.reviews[-count:]\n",
        "    \n",
        "    def search_reviews(self, keyword: str) -> List[Dict[str, Any]]:\n",
        "        keyword_lower = keyword.lower()\n",
        "        results = []\n",
        "        for review in self.reviews:\n",
        "            review_str = json.dumps(review).lower()\n",
        "            if keyword_lower in review_str:\n",
        "                results.append(review)\n",
        "        return results\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        if not self.reviews:\n",
        "            return {\"total_reviews\": 0, \"avg_scores\": {}}\n",
        "        \n",
        "        # Calculate average scores\n",
        "        scores = {\"storytelling\": [], \"double_diamond\": [], \"designer_influence\": [], \"collaboration\": []}\n",
        "        for review in self.reviews:\n",
        "            if \"evaluations\" in review:\n",
        "                evals = review[\"evaluations\"]\n",
        "                if \"storytelling\" in evals and \"score\" in evals[\"storytelling\"]:\n",
        "                    scores[\"storytelling\"].append(evals[\"storytelling\"][\"score\"])\n",
        "                if \"double_diamond\" in evals and \"score\" in evals[\"double_diamond\"]:\n",
        "                    scores[\"double_diamond\"].append(evals[\"double_diamond\"][\"score\"])\n",
        "                if \"designer_influence\" in evals and \"score\" in evals[\"designer_influence\"]:\n",
        "                    scores[\"designer_influence\"].append(evals[\"designer_influence\"][\"score\"])\n",
        "                if \"collaboration\" in evals and \"score\" in evals[\"collaboration\"]:\n",
        "                    scores[\"collaboration\"].append(evals[\"collaboration\"][\"score\"])\n",
        "        \n",
        "        avg_scores = {}\n",
        "        for key, values in scores.items():\n",
        "            if values:\n",
        "                avg_scores[key] = sum(values) / len(values)\n",
        "        \n",
        "        return {\n",
        "            \"total_reviews\": len(self.reviews),\n",
        "            \"avg_scores\": avg_scores\n",
        "        }\n",
        "    \n",
        "    def clear(self):\n",
        "        self.reviews.clear()\n",
        "\n",
        "\n",
        "memory = ConversationMemory(max_history=20)\n",
        "review_memory = PortfolioReviewMemory(max_reviews=50)\n",
        "print(f\"âœ“ Memory System Initialized\")\n",
        "print(f\"  â€¢ Conversation Memory: Max {memory.max_history} messages\")\n",
        "print(f\"  â€¢ Review Memory: Max {review_memory.max_reviews} reviews\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AgentLogger:\n",
        "    \"\"\"Comprehensive logging for agent operations\"\"\"\n",
        "    logs: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    \n",
        "    def log(self, level: str, event: str, details: Dict[str, Any] = None):\n",
        "        self.logs.append({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"level\": level,\n",
        "            \"event\": event,\n",
        "            \"details\": details or {}\n",
        "        })\n",
        "    \n",
        "    def info(self, event: str, **kwargs):\n",
        "        self.log(\"INFO\", event, kwargs)\n",
        "    \n",
        "    def error(self, event: str, **kwargs):\n",
        "        self.log(\"ERROR\", event, kwargs)\n",
        "    \n",
        "    def warning(self, event: str, **kwargs):\n",
        "        self.log(\"WARNING\", event, kwargs)\n",
        "    \n",
        "    def get_recent_logs(self, count: int = 10) -> List[Dict]:\n",
        "        return self.logs[-count:]\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"total_logs\": len(self.logs),\n",
        "            \"info_count\": sum(1 for log in self.logs if log['level'] == 'INFO'),\n",
        "            \"error_count\": sum(1 for log in self.logs if log['level'] == 'ERROR'),\n",
        "            \"warning_count\": sum(1 for log in self.logs if log['level'] == 'WARNING')\n",
        "        }\n",
        "    \n",
        "    def export_logs(self, filename: str = \"agent_logs.json\"):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        print(f\"âœ“ Logs exported to {filename}\")\n",
        "\n",
        "\n",
        "logger = AgentLogger()\n",
        "logger.info(\"Logger initialized\")\n",
        "print(\"âœ“ Logging System Ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specialized Agent Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StorytellingAgent:\n",
        "    \"\"\"Specialized agent for evaluating storytelling aspects\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, logger: AgentLogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.name = \"StorytellingAgent\"\n",
        "    \n",
        "    def evaluate(self, content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate storytelling quality\"\"\"\n",
        "        self.logger.info(f\"{self.name} evaluation started\", content_length=len(content))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            result = evaluate_storytelling(content, context)\n",
        "            elapsed = time.time() - start_time\n",
        "            self.logger.info(f\"{self.name} evaluation completed\", \n",
        "                           score=result.get(\"score\", 0), \n",
        "                           elapsed=f\"{elapsed:.2f}s\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"{self.name} evaluation failed\", error=str(e))\n",
        "            return {\"score\": 0, \"error\": str(e)}\n",
        "\n",
        "\n",
        "class DoubleDiamondAgent:\n",
        "    \"\"\"Specialized agent for evaluating double-diamond design process\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, logger: AgentLogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.name = \"DoubleDiamondAgent\"\n",
        "    \n",
        "    def evaluate(self, content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate double-diamond process evidence\"\"\"\n",
        "        self.logger.info(f\"{self.name} evaluation started\", content_length=len(content))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            result = evaluate_double_diamond(content, context)\n",
        "            elapsed = time.time() - start_time\n",
        "            self.logger.info(f\"{self.name} evaluation completed\", \n",
        "                           score=result.get(\"score\", 0), \n",
        "                           elapsed=f\"{elapsed:.2f}s\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"{self.name} evaluation failed\", error=str(e))\n",
        "            return {\"score\": 0, \"error\": str(e)}\n",
        "\n",
        "\n",
        "class DesignerInfluenceAgent:\n",
        "    \"\"\"Specialized agent for evaluating designer influence and decision-making\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, logger: AgentLogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.name = \"DesignerInfluenceAgent\"\n",
        "    \n",
        "    def evaluate(self, content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate designer influence evidence\"\"\"\n",
        "        self.logger.info(f\"{self.name} evaluation started\", content_length=len(content))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            result = evaluate_designer_influence(content, context)\n",
        "            elapsed = time.time() - start_time\n",
        "            self.logger.info(f\"{self.name} evaluation completed\", \n",
        "                           score=result.get(\"score\", 0), \n",
        "                           elapsed=f\"{elapsed:.2f}s\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"{self.name} evaluation failed\", error=str(e))\n",
        "            return {\"score\": 0, \"error\": str(e)}\n",
        "\n",
        "\n",
        "class CollaborationAgent:\n",
        "    \"\"\"Specialized agent for evaluating cross-discipline collaboration\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, logger: AgentLogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.name = \"CollaborationAgent\"\n",
        "    \n",
        "    def evaluate(self, content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate collaboration evidence\"\"\"\n",
        "        self.logger.info(f\"{self.name} evaluation started\", content_length=len(content))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            result = evaluate_collaboration(content, context)\n",
        "            elapsed = time.time() - start_time\n",
        "            self.logger.info(f\"{self.name} evaluation completed\", \n",
        "                           score=result.get(\"score\", 0), \n",
        "                           elapsed=f\"{elapsed:.2f}s\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"{self.name} evaluation failed\", error=str(e))\n",
        "            return {\"score\": 0, \"error\": str(e)}\n",
        "\n",
        "\n",
        "class ReportGeneratorAgent:\n",
        "    \"\"\"Specialized agent for synthesizing evaluations into comprehensive report\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, logger: AgentLogger):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.name = \"ReportGeneratorAgent\"\n",
        "    \n",
        "    def generate_report(self, evaluations: Dict[str, Any], improvement_plan: str) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive review report\"\"\"\n",
        "        self.logger.info(f\"{self.name} report generation started\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Calculate overall score\n",
        "            scores = []\n",
        "            for key, eval_result in evaluations.items():\n",
        "                if isinstance(eval_result, dict) and \"score\" in eval_result:\n",
        "                    scores.append(eval_result[\"score\"])\n",
        "            \n",
        "            overall_score = sum(scores) / len(scores) if scores else 0\n",
        "            \n",
        "            report = {\n",
        "                \"overall_score\": round(overall_score, 2),\n",
        "                \"evaluations\": evaluations,\n",
        "                \"improvement_plan\": improvement_plan,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"summary\": {\n",
        "                    \"storytelling_score\": evaluations.get(\"storytelling\", {}).get(\"score\", 0),\n",
        "                    \"double_diamond_score\": evaluations.get(\"double_diamond\", {}).get(\"score\", 0),\n",
        "                    \"designer_influence_score\": evaluations.get(\"designer_influence\", {}).get(\"score\", 0),\n",
        "                    \"collaboration_score\": evaluations.get(\"collaboration\", {}).get(\"score\", 0)\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            self.logger.info(f\"{self.name} report generated\", \n",
        "                           overall_score=overall_score, \n",
        "                           elapsed=f\"{elapsed:.2f}s\")\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"{self.name} report generation failed\", error=str(e))\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "print(\"âœ“ Specialized Agent Classes Created\")\n",
        "print(\"  â€¢ StorytellingAgent\")\n",
        "print(\"  â€¢ DoubleDiamondAgent\")\n",
        "print(\"  â€¢ DesignerInfluenceAgent\")\n",
        "print(\"  â€¢ CollaborationAgent\")\n",
        "print(\"  â€¢ ReportGeneratorAgent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PortfolioReviewCoordinator:\n",
        "    \"\"\"Main orchestrating agent that coordinates multi-agent review process\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, tools: Tool, memory: ConversationMemory, \n",
        "                 review_memory: PortfolioReviewMemory, logger: AgentLogger):\n",
        "        self.config = config\n",
        "        self.tools = tools\n",
        "        self.memory = memory\n",
        "        self.review_memory = review_memory\n",
        "        self.logger = logger\n",
        "        \n",
        "        # Initialize specialized agents\n",
        "        self.storytelling_agent = StorytellingAgent(config, logger)\n",
        "        self.double_diamond_agent = DoubleDiamondAgent(config, logger)\n",
        "        self.designer_influence_agent = DesignerInfluenceAgent(config, logger)\n",
        "        self.collaboration_agent = CollaborationAgent(config, logger)\n",
        "        self.report_generator_agent = ReportGeneratorAgent(config, logger)\n",
        "        \n",
        "        self.stats = {\n",
        "            \"reviews_processed\": 0,\n",
        "            \"total_response_time\": 0.0,\n",
        "            \"errors\": 0\n",
        "        }\n",
        "        \n",
        "        self.logger.info(\"PortfolioReviewCoordinator initialized\")\n",
        "    \n",
        "    def _call_function(self, function_call) -> str:\n",
        "        \"\"\"Execute tool function and return result\"\"\"\n",
        "        function_name = function_call.name\n",
        "        function_args = dict(function_call.args)\n",
        "        \n",
        "        self.logger.info(\"Function called\", function=function_name, args=str(function_args))\n",
        "        \n",
        "        function_map = {\n",
        "            \"fetch_url_content\": fetch_url_content,\n",
        "            \"parse_portfolio_content\": parse_portfolio_content,\n",
        "            \"evaluate_storytelling\": evaluate_storytelling,\n",
        "            \"evaluate_double_diamond\": evaluate_double_diamond,\n",
        "            \"evaluate_designer_influence\": evaluate_designer_influence,\n",
        "            \"evaluate_collaboration\": evaluate_collaboration,\n",
        "            \"generate_improvement_plan\": generate_improvement_plan\n",
        "        }\n",
        "        \n",
        "        if function_name in function_map:\n",
        "            try:\n",
        "                result = function_map[function_name](**function_args)\n",
        "                return str(result) if not isinstance(result, str) else result\n",
        "            except Exception as e:\n",
        "                self.logger.error(\"Function execution failed\", error=str(e))\n",
        "                return f\"Error executing {function_name}: {str(e)}\"\n",
        "        return f\"Unknown function: {function_name}\"\n",
        "    \n",
        "    def review_portfolio(self, content: str, input_type: str = \"text\") -> Dict[str, Any]:\n",
        "        \"\"\"Orchestrate multi-agent portfolio review\"\"\"\n",
        "        start_time = time.time()\n",
        "        self.logger.info(\"Portfolio review started\", input_type=input_type, content_length=len(content))\n",
        "        \n",
        "        try:\n",
        "            # Parse content if needed\n",
        "            parsed_content = parse_portfolio_content(content)\n",
        "            content_text = parsed_content.get(\"raw_content\", content)\n",
        "            \n",
        "            # Sequential evaluation by specialized agents\n",
        "            evaluations = {}\n",
        "            \n",
        "            # 1. Storytelling evaluation\n",
        "            self.logger.info(\"Starting storytelling evaluation\")\n",
        "            evaluations[\"storytelling\"] = self.storytelling_agent.evaluate(content_text)\n",
        "            \n",
        "            # 2. Double-diamond evaluation\n",
        "            self.logger.info(\"Starting double-diamond evaluation\")\n",
        "            evaluations[\"double_diamond\"] = self.double_diamond_agent.evaluate(content_text)\n",
        "            \n",
        "            # 3. Designer influence evaluation\n",
        "            self.logger.info(\"Starting designer influence evaluation\")\n",
        "            evaluations[\"designer_influence\"] = self.designer_influence_agent.evaluate(content_text)\n",
        "            \n",
        "            # 4. Collaboration evaluation\n",
        "            self.logger.info(\"Starting collaboration evaluation\")\n",
        "            evaluations[\"collaboration\"] = self.collaboration_agent.evaluate(content_text)\n",
        "            \n",
        "            # 5. Generate improvement plan\n",
        "            self.logger.info(\"Generating improvement plan\")\n",
        "            improvement_plan = generate_improvement_plan(evaluations)\n",
        "            \n",
        "            # 6. Generate comprehensive report\n",
        "            self.logger.info(\"Generating final report\")\n",
        "            report = self.report_generator_agent.generate_report(evaluations, improvement_plan)\n",
        "            \n",
        "            # Store in review memory\n",
        "            review_data = {\n",
        "                \"input_type\": input_type,\n",
        "                \"content_length\": len(content),\n",
        "                \"evaluations\": evaluations,\n",
        "                \"report\": report\n",
        "            }\n",
        "            self.review_memory.add_review(review_data)\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            self.stats[\"reviews_processed\"] += 1\n",
        "            self.stats[\"total_response_time\"] += elapsed\n",
        "            \n",
        "            self.logger.info(\"Portfolio review completed\", \n",
        "                           overall_score=report.get(\"overall_score\", 0),\n",
        "                           elapsed=f\"{elapsed:.2f}s\")\n",
        "            \n",
        "            return report\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.stats[\"errors\"] += 1\n",
        "            self.logger.error(\"Portfolio review failed\", error=str(e))\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"overall_score\": 0,\n",
        "                \"evaluations\": {},\n",
        "                \"improvement_plan\": \"Review failed. Please try again.\"\n",
        "            }\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get coordinator statistics\"\"\"\n",
        "        avg_time = (\n",
        "            self.stats[\"total_response_time\"] / self.stats[\"reviews_processed\"]\n",
        "            if self.stats[\"reviews_processed\"] > 0 else 0\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            **self.stats,\n",
        "            \"avg_response_time\": round(avg_time, 2),\n",
        "            \"memory_stats\": self.memory.get_stats(),\n",
        "            \"review_memory_stats\": self.review_memory.get_stats(),\n",
        "            \"logger_stats\": self.logger.get_stats()\n",
        "        }\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset coordinator state\"\"\"\n",
        "        self.memory.clear()\n",
        "        self.stats = {\"reviews_processed\": 0, \"total_response_time\": 0.0, \"errors\": 0}\n",
        "        self.logger.info(\"Coordinator reset\")\n",
        "\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    coordinator = PortfolioReviewCoordinator(\n",
        "        config=CONFIG,\n",
        "        tools=tools,\n",
        "        memory=memory,\n",
        "        review_memory=review_memory,\n",
        "        logger=logger\n",
        "    )\n",
        "    print(\"âœ“ Coordinator Agent Initialized\")\n",
        "    print(\"âœ“ Ready for Portfolio Reviews\")\n",
        "else:\n",
        "    coordinator = None\n",
        "    print(\"âš  Coordinator initialization skipped - Configure API key\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def review_portfolio(input_source: str, input_type: str = \"auto\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Main function to review a design portfolio.\n",
        "    \n",
        "    Args:\n",
        "        input_source: URL or text content of the portfolio\n",
        "        input_type: 'url', 'text', or 'auto' (auto-detect)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing comprehensive review report\n",
        "    \"\"\"\n",
        "    if not coordinator:\n",
        "        return {\"error\": \"Coordinator not initialized. Please configure API key.\"}\n",
        "    \n",
        "    # Auto-detect input type\n",
        "    if input_type == \"auto\":\n",
        "        if input_source.startswith(\"http://\") or input_source.startswith(\"https://\"):\n",
        "            input_type = \"url\"\n",
        "        else:\n",
        "            input_type = \"text\"\n",
        "    \n",
        "    # Fetch content if URL\n",
        "    if input_type == \"url\":\n",
        "        print(f\"Fetching content from URL: {input_source}\")\n",
        "        content = fetch_url_content(input_source)\n",
        "        if content.startswith(\"Error\"):\n",
        "            return {\"error\": content}\n",
        "    else:\n",
        "        content = input_source\n",
        "    \n",
        "    # Review portfolio\n",
        "    print(f\"Starting portfolio review (type: {input_type}, length: {len(content)} chars)...\")\n",
        "    report = coordinator.review_portfolio(content, input_type)\n",
        "    \n",
        "    return report\n",
        "\n",
        "\n",
        "print(\"âœ“ Main Review Function Ready\")\n",
        "print(\"ðŸ“Œ Usage: review_portfolio('https://example.com/portfolio') or review_portfolio('portfolio text here')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_review_report(report: Dict[str, Any]):\n",
        "    \"\"\"Pretty-print evaluation results\"\"\"\n",
        "    if \"error\" in report:\n",
        "        print(f\"âŒ Error: {report['error']}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{'PORTFOLIO REVIEW REPORT':^60}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Overall score\n",
        "    overall = report.get(\"overall_score\", 0)\n",
        "    print(f\"ðŸ“Š Overall Score: {overall}/100\\n\")\n",
        "    \n",
        "    # Summary scores\n",
        "    summary = report.get(\"summary\", {})\n",
        "    print(\"ðŸ“ˆ Category Scores:\")\n",
        "    print(f\"  â€¢ Storytelling: {summary.get('storytelling_score', 0)}/100\")\n",
        "    print(f\"  â€¢ Double-Diamond Process: {summary.get('double_diamond_score', 0)}/100\")\n",
        "    print(f\"  â€¢ Designer Influence: {summary.get('designer_influence_score', 0)}/100\")\n",
        "    print(f\"  â€¢ Collaboration: {summary.get('collaboration_score', 0)}/100\\n\")\n",
        "    \n",
        "    # Detailed evaluations\n",
        "    evaluations = report.get(\"evaluations\", {})\n",
        "    for category, eval_result in evaluations.items():\n",
        "        if isinstance(eval_result, dict):\n",
        "            print(f\"{'='*60}\")\n",
        "            print(f\"{category.upper().replace('_', ' ')}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            print(f\"Score: {eval_result.get('score', 0)}/100\\n\")\n",
        "            \n",
        "            if \"strengths\" in eval_result:\n",
        "                print(\"âœ… Strengths:\")\n",
        "                for strength in eval_result[\"strengths\"]:\n",
        "                    print(f\"  â€¢ {strength}\")\n",
        "                print()\n",
        "            \n",
        "            if \"weaknesses\" in eval_result:\n",
        "                print(\"âš ï¸ Weaknesses:\")\n",
        "                for weakness in eval_result[\"weaknesses\"]:\n",
        "                    print(f\"  â€¢ {weakness}\")\n",
        "                print()\n",
        "            \n",
        "            if \"recommendations\" in eval_result:\n",
        "                print(\"ðŸ’¡ Recommendations:\")\n",
        "                for rec in eval_result[\"recommendations\"]:\n",
        "                    print(f\"  â€¢ {rec}\")\n",
        "                print()\n",
        "    \n",
        "    # Improvement plan\n",
        "    if \"improvement_plan\" in report:\n",
        "        print(f\"{'='*60}\")\n",
        "        print(\"IMPROVEMENT PLAN\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        print(report[\"improvement_plan\"])\n",
        "        print()\n",
        "    \n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "\n",
        "def export_review_report(report: Dict[str, Any], filename: str = \"portfolio_review.json\"):\n",
        "    \"\"\"Export review report to JSON file\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        print(f\"âœ“ Review report exported to: {filename}\")\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error exporting report: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def search_review_history(keyword: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search past portfolio reviews\"\"\"\n",
        "    if not coordinator:\n",
        "        print(\"âš  Coordinator not initialized\")\n",
        "        return []\n",
        "    \n",
        "    results = coordinator.review_memory.search_reviews(keyword)\n",
        "    \n",
        "    if results:\n",
        "        print(f\"ðŸ” Found {len(results)} review(s) containing '{keyword}':\\n\")\n",
        "        for idx, review in enumerate(results, 1):\n",
        "            print(f\"Review #{idx} [{review.get('timestamp', 'N/A')}]\")\n",
        "            if \"report\" in review and \"overall_score\" in review[\"report\"]:\n",
        "                print(f\"  Overall Score: {review['report']['overall_score']}/100\")\n",
        "            print(\"-\" * 60)\n",
        "    else:\n",
        "        print(f\"âŒ No reviews found containing '{keyword}'\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def get_review_statistics() -> Dict[str, Any]:\n",
        "    \"\"\"Get analytics on reviews performed\"\"\"\n",
        "    if not coordinator:\n",
        "        print(\"âš  Coordinator not initialized\")\n",
        "        return {}\n",
        "    \n",
        "    stats = coordinator.get_stats()\n",
        "    review_stats = coordinator.review_memory.get_stats()\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{'REVIEW STATISTICS':^60}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    print(\"ðŸ“Š Review Performance:\")\n",
        "    print(f\"  Total Reviews: {stats['reviews_processed']}\")\n",
        "    print(f\"  Avg Response Time: {stats['avg_response_time']:.2f}s\")\n",
        "    print(f\"  Errors: {stats['errors']}\\n\")\n",
        "    \n",
        "    print(\"ðŸ“ˆ Review History:\")\n",
        "    print(f\"  Total Reviews Stored: {review_stats['total_reviews']}\")\n",
        "    if review_stats['avg_scores']:\n",
        "        print(\"  Average Scores:\")\n",
        "        for category, avg_score in review_stats['avg_scores'].items():\n",
        "            print(f\"    â€¢ {category.replace('_', ' ').title()}: {avg_score:.1f}/100\")\n",
        "    print()\n",
        "    \n",
        "    print(\"ðŸ’­ Memory Statistics:\")\n",
        "    mem_stats = stats['memory_stats']\n",
        "    print(f\"  Total Messages: {mem_stats['total_messages']}\")\n",
        "    print(f\"  User Messages: {mem_stats['user_messages']}\")\n",
        "    print(f\"  Agent Messages: {mem_stats['agent_messages']}\\n\")\n",
        "    \n",
        "    print(\"ðŸ“ Logger Statistics:\")\n",
        "    log_stats = stats['logger_stats']\n",
        "    print(f\"  Total Logs: {log_stats['total_logs']}\")\n",
        "    print(f\"  Info: {log_stats['info_count']} | Warning: {log_stats['warning_count']} | Error: {log_stats['error_count']}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "\n",
        "print(\"âœ“ Utility Functions Ready\")\n",
        "print(\"  â€¢ display_review_report(report)\")\n",
        "print(\"  â€¢ export_review_report(report, filename)\")\n",
        "print(\"  â€¢ search_review_history(keyword)\")\n",
        "print(\"  â€¢ get_review_statistics()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo & Testing\n",
        "\n",
        "### Example 1: Review Portfolio from Text/Markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample portfolio case study text\n",
        "sample_portfolio_text = \"\"\"\n",
        "# E-Commerce Mobile App Redesign\n",
        "\n",
        "## Problem Statement\n",
        "Our e-commerce mobile app had low user engagement and high cart abandonment rates. \n",
        "User research revealed that the checkout process was confusing and the product discovery \n",
        "was limited.\n",
        "\n",
        "## Discovery Phase\n",
        "I conducted 15 user interviews to understand pain points. Key findings:\n",
        "- 60% of users abandoned carts due to complex checkout\n",
        "- Users wanted better product filtering\n",
        "- Mobile experience was slow and clunky\n",
        "\n",
        "## Define Phase\n",
        "We synthesized insights into three key opportunities:\n",
        "1. Simplify checkout to 3 steps max\n",
        "2. Improve product search and filtering\n",
        "3. Optimize mobile performance\n",
        "\n",
        "## Develop Phase\n",
        "I created wireframes and prototypes in Figma. We tested with 5 users iteratively.\n",
        "Key decisions I made:\n",
        "- Removed unnecessary form fields\n",
        "- Added one-click checkout option\n",
        "- Implemented smart product recommendations\n",
        "\n",
        "## Deliver Phase\n",
        "Launched the redesigned app. Results:\n",
        "- Cart abandonment reduced by 40%\n",
        "- User engagement increased by 25%\n",
        "- App store rating improved from 3.2 to 4.5\n",
        "\n",
        "## Collaboration\n",
        "Worked closely with:\n",
        "- Engineering team (2 developers)\n",
        "- Product Manager\n",
        "- UX Researcher\n",
        "- Marketing team for launch\n",
        "\n",
        "We used daily standups, design reviews, and user testing sessions to align.\n",
        "\"\"\"\n",
        "\n",
        "# Review the sample portfolio\n",
        "if coordinator:\n",
        "    print(\"=\"*60)\n",
        "    print(\"DEMO 1: Reviewing Sample Portfolio (Text Input)\")\n",
        "    print(\"=\"*60)\n",
        "    report = review_portfolio(sample_portfolio_text, input_type=\"text\")\n",
        "    display_review_report(report)\n",
        "else:\n",
        "    print(\"âš  Coordinator not initialized. Please configure API key first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Review Portfolio from URL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Review portfolio from URL\n",
        "# Uncomment and replace with actual portfolio URL to test\n",
        "# portfolio_url = \"https://example.com/designer-portfolio\"\n",
        "\n",
        "# if coordinator:\n",
        "#     print(\"=\"*60)\n",
        "#     print(\"DEMO 2: Reviewing Portfolio from URL\")\n",
        "#     print(\"=\"*60)\n",
        "#     report = review_portfolio(portfolio_url, input_type=\"url\")\n",
        "#     display_review_report(report)\n",
        "#     \n",
        "#     # Export the report\n",
        "#     export_review_report(report, \"url_portfolio_review.json\")\n",
        "# else:\n",
        "#     print(\"âš  Coordinator not initialized. Please configure API key first.\")\n",
        "\n",
        "print(\"ðŸ“Œ To review a portfolio from URL, uncomment the code above and provide a valid URL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display performance statistics\n",
        "if coordinator:\n",
        "    get_review_statistics()\n",
        "else:\n",
        "    print(\"âš  Coordinator not initialized. Please configure API key first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Summary\n",
        "\n",
        "### System Architecture\n",
        "\n",
        "This multi-agent system consists of:\n",
        "\n",
        "1. **Coordinator Agent** - Orchestrates the entire review process\n",
        "2. **Storytelling Agent** - Evaluates narrative structure and engagement\n",
        "3. **Double-Diamond Agent** - Assesses design process evidence\n",
        "4. **Designer Influence Agent** - Analyzes decision-making and leadership\n",
        "5. **Collaboration Agent** - Evaluates teamwork and stakeholder engagement\n",
        "6. **Report Generator Agent** - Synthesizes all evaluations\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- âœ… Multi-agent orchestration with sequential evaluation\n",
        "- âœ… Custom tools for URL fetching and content parsing\n",
        "- âœ… Comprehensive evaluation across 4 key dimensions\n",
        "- âœ… Actionable improvement plans\n",
        "- âœ… Review history and analytics\n",
        "- âœ… Export capabilities (JSON reports)\n",
        "- âœ… Comprehensive logging and observability\n",
        "\n",
        "### Usage\n",
        "\n",
        "```python\n",
        "# Review from text\n",
        "report = review_portfolio(\"Your portfolio text here\")\n",
        "\n",
        "# Review from URL\n",
        "report = review_portfolio(\"https://example.com/portfolio\", input_type=\"url\")\n",
        "\n",
        "# Display results\n",
        "display_review_report(report)\n",
        "\n",
        "# Export report\n",
        "export_review_report(report, \"my_review.json\")\n",
        "\n",
        "# Search history\n",
        "search_review_history(\"keyword\")\n",
        "\n",
        "# Get statistics\n",
        "get_review_statistics()\n",
        "```\n",
        "\n",
        "### Evaluation Criteria\n",
        "\n",
        "Each portfolio is evaluated on:\n",
        "\n",
        "1. **Storytelling** (0-100): Narrative structure, problem clarity, solution journey, engagement\n",
        "2. **Double-Diamond Process** (0-100): Evidence of discover, define, develop, deliver phases\n",
        "3. **Designer Influence** (0-100): Decision-making, conflict resolution, leadership, rationale\n",
        "4. **Collaboration** (0-100): Cross-functional teamwork, stakeholder engagement, feedback incorporation\n",
        "\n",
        "### Output\n",
        "\n",
        "Each review provides:\n",
        "- Overall score (average of all categories)\n",
        "- Category-specific scores\n",
        "- Strengths identified\n",
        "- Weaknesses/gaps\n",
        "- Specific evidence found\n",
        "- Actionable improvement recommendations\n",
        "- Prioritized improvement plan\n",
        "\n",
        "---\n",
        "\n",
        "**Track:** Agents for Good (Education)  \n",
        "**Course:** 5-Day AI Agents Intensive with Google  \n",
        "**Date:** November 2025\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
